# Awesome-Vision-Token-Pruning-for-VLMs
# Accepted Paper
|Date|Paper|Author|Conference|
|:---:|:---:|:---:|:---:| 
|ECCV2024 accepted|[IVTP: Instruction-Guided Visual Token Pruning for Large Vision-Language Models](https://link.springer.com/chapter/10.1007/978-3-031-72643-9_13)	|Alibaba Group	|ECCV2024 accepted|
|ECCV2024 accepted|[An Image is Worth 1/2 Tokens After Layer 2: Plug-and-Play Inference Acceleration for Large Vision-Language Models](https://link.springer.com/chapter/10.1007/978-3-031-73004-7_2)	|Peking University	|ECCV2024 accepted|
|ECCV2024 accepted|[Efficient Inference of Vision Instruction-Following Models with Elastic Cache](https://link.springer.com/chapter/10.1007/978-3-031-72643-9_4)	|Tsinghua University	|ECCV2024 accepted|
|ECCV2024 accepted|[Leveraging Pruning, Quantization and Multi-objective Optimization for an Efficient Deployment of Multi-modal Models](https://link.springer.com/chapter/10.1007/978-981-96-2074-6_5)|University of Science |ECCV2024 accepted|
|ECCV2024 accepted|[Turbo: Informativity-Driven Acceleration Plug-In for Vision-Language Large Models](https://link.springer.com/chapter/10.1007/978-3-031-72952-2_25)|Alibaba Group|ECCV2024 accepted|
|CVPR2024 accepted|[MADTP: Multimodal Alignment-Guided Dynamic Token Pruning for Accelerating Vision-Language Transformer](https://openaccess.thecvf.com/content/CVPR2024/html/Cao_MADTP_Multimodal_Alignment-Guided_Dynamic_Token_Pruning_for_Accelerating_Vision-Language_Transformer_CVPR_2024_paper.html)|Fudan University|CVPR2024 accepted|
|CVPR2024 accepted|[Zero-TPrune: Zero-Shot Token Pruning through Leveraging of the Attention Graph in Pre-Trained Transformers](https://openaccess.thecvf.com/content/CVPR2024/html/Wang_Zero-TPrune_Zero-Shot_Token_Pruning_through_Leveraging_of_the_Attention_Graph_CVPR_2024_paper.html)|Princeton University|CVPR2024 accepted|
|CVPR2024 accepted|[MULTIFLOW: Shifting Towards Task-Agnostic Vision-Language Pruning](https://openaccess.thecvf.com/content/CVPR2024/html/Farina_MULTIFLOW_Shifting_Towards_Task-Agnostic_Vision-Language_Pruning_CVPR_2024_paper.html)|University of Trento|CVPR2024 accepted|
|CVPR2024 accepted|[MoPE-CLIP: Structured Pruning for Efficient Vision-Language Models with Module-wise Pruning Error Metric](https://openaccess.thecvf.com/content/CVPR2024/html/Lin_MoPE-CLIP_Structured_Pruning_for_Efficient_Vision-Language_Models_with_Module-wise_Pruning_CVPR_2024_paper.html)|University of Chinese Academy of Sciences|CVPR2024 accepted|
|CVPR2024 accepted|[Resource-Efficient Transformer Pruning for Finetuning of Large Models](https://openaccess.thecvf.com/content/CVPR2024/html/Ilhan_Resource-Efficient_Transformer_Pruning_for_Finetuning_of_Large_Models_CVPR_2024_paper.html)|Georgia Institute of Technology|CVPR2024 accepted|
|AAAI2025 accepted|[ST3: Accelerating Multimodal Large Language Model by Spatial-Temporal Visual Token Trimming](https://arxiv.org/abs/2412.20105)|Zhejiang University|AAAI2025 accepted|
|AAAI2025 accepted|[HiRED: Attention-Guided Token Dropping for Efficient Inference of High-Resolution Vision-Language Models](https://arxiv.org/abs/2408.10945)|Virginia Tech|AAAI2025 accepted|
|COLING2025 accepted|[Less is More: A Simple yet Effective Token Reduction Method for Efficient Multi-modal LLMs](https://arxiv.org/abs/2409.10994)|The Chinese University of Hong Kong|COLING2025 accepted|
|NeurIPS2024 accepted|[Accelerating Transformers with Spectrum-Preserving Token Merging](https://arxiv.org/abs/2405.16148)|German Research Center for Artificial Intelligence|NeurIPS2024 accepted|
|ICML2024 accepted|[OSSCAR: One-Shot Structured Pruning in Vision and Language Models with Combinatorial Optimization](https://arxiv.org/abs/2403.12983)|MIT|ICML2024 accepted|
|ACL2023 accepted|[PuMer: Pruning and Merging Tokens for Efficient Vision Language Models](https://arxiv.org/abs/2305.17530)|University of Washington||
|EMNLP2024 accepted|[LOOK-M: Look-Once Optimization in KV Cache for Efficient Multimodal Long-Context Inference](https://arxiv.org/abs/2406.18139)|The Ohio State University|EMNLP2024 accepted|


# 2025.1
|Date|Paper|Author|Conference|
|:---:|:---:|:---:|:---:| 
|2025.1	|[What Kind of Visual Tokens Do We Need? Training-free Visual Token Pruning for Multi-modal Large Language Models from the Perspective of Graph](https://arxiv.org/abs/2501.02268)	|Xiamen University	||
|2025.1	|[AdaFV: Accelerating VLMs with Self-Adaptive Cross-Modality Attention Mixture](https://arxiv.org/abs/2501.09532)|Inspur Genersoft Co. Ltd.||
|2025.1 |[FOLDER: Accelerating Multi-modal Large Language Models with Enhanced Performance](https://arxiv.org/abs/2501.02430)| Shanghai Jiao Tong University||
|2025.1 |[Compression with Global Guidance: Towards Training-free High-Resolution MLLMs Acceleration](https://arxiv.org/abs/2501.05179)|Sichuan University||


# 2024.12
|Date|Paper|Author|Conference|
|:---:|:---:|:---:|:---:| 
|2024.12|[[CLS] Attention is All You Need for Training-Free Visual Token Pruning: Make VLM Inference Faster](https://arxiv.org/abs/2412.01818)|Peking University|ICML2025|
|2024.12|[AIM: Adaptive Inference of Multi-Modal LLMs via Token Merging and Pruning](https://arxiv.org/abs/2412.03248)|The Chinese University of Hong Kong||
|2024.12|[Dynamic-LLaVA: Efficient Multimodal Large Language Models via Dynamic Vision-language Context Sparsification](https://arxiv.org/abs/2412.00876)|East China Normal University||
|2024.12|[iLLaVA: An Image is Worth Fewer Than 1/3 Input Tokens in Large Multimodal Models](https://arxiv.org/abs/2412.06263)|Tianjin University||
|2024.12|[VisionZip: Longer is Better but Not Necessary in Vision Language Models](https://arxiv.org/abs/2412.04467)|CUHK||
|2024.12|[NVILA: Efficient Frontier Visual Language Models](https://arxiv.org/abs/2412.04468)|NVIDIA||
|2024.12|[PrefixKV: Adaptive Prefix KV Cache is What Vision Instruction-Following Models Need for Efficient Generation](https://arxiv.org/abs/2412.03409)|Tsinghua University||
|2024.12|[Dynamic-VLM: Simple Dynamic Visual Token Compression for VideoLLM](https://arxiv.org/abs/2412.09530)|Bytedance Inc.||
|2024.12|[Feather the Throttle: Revisiting Visual Token Pruning for Vision-Language Model Acceleration](https://arxiv.org/abs/2412.13180)|Stanford University||
|2024.12|[PruneVid: Visual Token Pruning for Efficient Video Large Language Models](https://arxiv.org/abs/2412.16117)|The University of Hong Kong||
|2024.12|[MBQ: Modality-Balanced Quantization for Large Vision-Language Models](https://arxiv.org/abs/2412.19509)|Tsinghua University||
|2024.12|[OmniVLM: A Token-Compressed, Sub-Billion-Parameter Vision-Language Model for Efficient On-Device Inference](https://arxiv.org/abs/2412.11475)|Nexa AI||
|2024.12|[Cross-Self KV Cache Pruning for Efficient Vision-Language Inference](https://arxiv.org/abs/2412.04652)|The University of Sydney||


# 2024.11
|Date|Paper|Author|Conference|
|:---:|:---:|:---:|:---:| 
|2024.11|[Multi-Stage Vision Token Dropping: Towards Efficient Multimodal Large Language Model](https://arxiv.org/abs/2411.10803)|National University of Defense Technology||
|2024.11|[Inference Optimal VLMs Need Only One Visual Token but Larger Models](https://arxiv.org/abs/2411.03312)|Carnegie Mellon University|ICLR2025 Submission|
|2024.11|[FocusLLaVA: A Coarse-to-Fine Approach for Efficient and Effective Visual Token Compression](https://arxiv.org/abs/2411.14228)|Ant Group||
|2024.11|[freePruner: A Training-free Approach for Large Multimodal Model Acceleration](https://arxiv.org/abs/2411.15446)|Illinois Institute of Technology||
|2024.11|[DyCoke: Dynamic Compression of Tokens for Fast Video Large Language Models](https://arxiv.org/abs/2411.15024)|Westlake University||


# 2024.10
|Date|Paper|Author|Conference|
|:---:|:---:|:---:|:---:| 
|2024.10|[SparseVLM: Visual Token Sparsification for Efficient Vision-Language Model Inference](https://arxiv.org/abs/2410.04417)|Peking University|ICLR2025 Submission|
|2024.10|[Rethinking Visual Dependency in Long-Context Reasoning for Large Vision-Language Models](https://arxiv.org/abs/2410.19732)|University of Macau||
|2024.10|[Treat Visual Tokens as Text? But Your MLLM Only Needs Fewer Efforts to See](https://arxiv.org/abs/2410.06169)|University of Rochester||
|2024.10|[ZipVL: Efficient Large Vision-Language Models with Dynamic Token Sparsification](https://arxiv.org/abs/2410.08584)|Zhejiang University||
|2024.10|[Efficient Vision-Language Models by Summarizing Visual Tokens into Compact Registers](https://arxiv.org/abs/2410.14072)|University of Maryland||
|2024.10|[PyramidDrop: Accelerating Your Large Vision-Language Models via Pyramid Visual Redundancy Reduction](https://arxiv.org/abs/2410.17247)|USTC|ICLR2025 Submission|
|2024.10|[VL-Cache: Sparsity and Modality-Aware KV Cache Compression for Vision-Language Model Inference Acceleration](https://arxiv.org/abs/2410.23317)|University of California|ICLR2025 Submission|
|2024.10|[xGen-MM-Vid (BLIP-3-Video): You Only Need 32 Tokens to Represent a Video Even in VLMs](https://arxiv.org/abs/2410.16267)|Salesforce AI Research|ICLR2025 Submission|


# 2024.9
|Date|Paper|Author|Conference|
|:---:|:---:|:---:|:---:| 
|2024.9|[Fit and Prune: Fast and Training-free Visual Token Pruning for Multi-modal Large Language Models](https://arxiv.org/abs/2409.10197)|Xiamen University	||
|2024.9|[Recoverable Compression: A Multimodal Vision Token Recovery Mechanism Guided by Text Information](https://arxiv.org/abs/2409.01179)|University of Chinese Academy of Sciences||
|2024.9|[Balancing Performance and Efficiency: A Multimodal Large Language Model Pruning Method based Image Text Interaction](https://arxiv.org/abs/2409.01162)|University of Chinese Academy of Sciences||
|2024.9|[Interpolating Video-LLMs: Toward Longer-sequence LMMs in a Training-free Manner](https://arxiv.org/abs/2409.12963)|Illinois Institute of Technology|ICLR2025 Submission|
|2024.9|[Inf-MLLM: Efficient Streaming Inference of Multimodal Large Language Models on a Single GPU](https://arxiv.org/abs/2409.09086)|Shanghai Jiao Tong University||


# 2024.6
|Date|Paper|Author|Conference|
|:---:|:---:|:---:|:---:| 
|2024.5|[VoCo-LLaMA: Towards Vision Compression with Large Language Models](https://arxiv.org/abs/2406.12275)|Tsinghua University||


# 2024.5
|Date|Paper|Author|Conference|
|:---:|:---:|:---:|:---:| 
|2024.5|[Boosting Multimodal Large Language Models with Visual Tokens Withdrawal for Rapid Inference](https://arxiv.org/abs/2405.05803)|Xiamen University||


# 2024.4
|Date|Paper|Author|Conference|
|:---:|:---:|:---:|:---:| 
|2024.4|[CATP: Cross-Attention Token Pruning for Accuracy Preserved Multimodal Model Inference](https://arxiv.org/abs/2404.08567)|Harvard University||


# 2024.3
|Date|Paper|Author|Conference|
|:---:|:---:|:---:|:---:| 
|2024.3|[LLaVA-PruMerge: Adaptive Token Reduction for Efficient Large Multimodal Models](https://arxiv.org/abs/2403.15388)|Illinois Institute of Technology|ICLR2025 Submission|


