# Awesome-Vision-Token-Pruning-for-VLMs
# Accepted Paper
|Date|Paper|Author|Conference|
|:---:|:---:|:---:|:---:| 
|ECCV2024 accepted|[IVTP: Instruction-Guided Visual Token Pruning for Large Vision-Language Models](https://link.springer.com/chapter/10.1007/978-3-031-72643-9_13)	|Alibaba Group	|ECCV2024 accepted|
|ECCV2024 accepted|[An Image is Worth 1/2 Tokens After Layer 2: Plug-and-Play Inference Acceleration for Large Vision-Language Models](https://link.springer.com/chapter/10.1007/978-3-031-73004-7_2)	|Peking University	|ECCV2024 accepted|
|ECCV2024 accepted|[Efficient Inference of Vision Instruction-Following Models with Elastic Cache](https://link.springer.com/chapter/10.1007/978-3-031-72643-9_4)	|Tsinghua University	|ECCV2024 accepted|
|ECCV2024 accepted|[Leveraging Pruning, Quantization and Multi-objective Optimization for an Efficient Deployment of Multi-modal Models](https://link.springer.com/chapter/10.1007/978-981-96-2074-6_5)|University of Science |ECCV2024 accepted|
|ECCV2024 accepted|[Turbo: Informativity-Driven Acceleration Plug-In for Vision-Language Large Models](https://link.springer.com/chapter/10.1007/978-3-031-72952-2_25)|Alibaba Group||ECCV2024 accepted|
|CVPR2024 accepted|[MADTP: Multimodal Alignment-Guided Dynamic Token Pruning for Accelerating Vision-Language Transformer](https://openaccess.thecvf.com/content/CVPR2024/html/Cao_MADTP_Multimodal_Alignment-Guided_Dynamic_Token_Pruning_for_Accelerating_Vision-Language_Transformer_CVPR_2024_paper.html)|Fudan University|CVPR2024 accepted|
|CVPR2024 accepted|[Zero-TPrune: Zero-Shot Token Pruning through Leveraging of the Attention Graph in Pre-Trained Transformers](https://openaccess.thecvf.com/content/CVPR2024/html/Wang_Zero-TPrune_Zero-Shot_Token_Pruning_through_Leveraging_of_the_Attention_Graph_CVPR_2024_paper.html)|Princeton University|CVPR2024 accepted|
|CVPR2024 accepted|[MULTIFLOW: Shifting Towards Task-Agnostic Vision-Language Pruning](https://openaccess.thecvf.com/content/CVPR2024/html/Farina_MULTIFLOW_Shifting_Towards_Task-Agnostic_Vision-Language_Pruning_CVPR_2024_paper.html)|University of Trento|CVPR2024 accepted|
|CVPR2024 accepted|[MoPE-CLIP: Structured Pruning for Efficient Vision-Language Models with Module-wise Pruning Error Metric](https://openaccess.thecvf.com/content/CVPR2024/html/Lin_MoPE-CLIP_Structured_Pruning_for_Efficient_Vision-Language_Models_with_Module-wise_Pruning_CVPR_2024_paper.html)|University of Chinese Academy of Sciences|CVPR2024 accepted|
|CVPR2024 accepted|[Resource-Efficient Transformer Pruning for Finetuning of Large Models](https://openaccess.thecvf.com/content/CVPR2024/html/Ilhan_Resource-Efficient_Transformer_Pruning_for_Finetuning_of_Large_Models_CVPR_2024_paper.html)|Georgia Institute of Technology|CVPR2024 accepted|
|AAAI2025 accepted|[ST3: Accelerating Multimodal Large Language Model by Spatial-Temporal Visual Token Trimming](https://arxiv.org/abs/2412.20105)|Zhejiang University|AAAI2025 accepted|
|AAAI2025 accepted|[HiRED: Attention-Guided Token Dropping for Efficient Inference of High-Resolution Vision-Language Models](https://arxiv.org/abs/2408.10945)|Virginia Tech|AAAI2025 accepted|
|COLING2025 accepted|[Less is More: A Simple yet Effective Token Reduction Method for Efficient Multi-modal LLMs](https://arxiv.org/abs/2409.10994)|The Chinese University of Hong Kong|COLING2025 accepted|
# 2025.1
|Date|Paper|Author|Conference|
|:---:|:---:|:---:|:---:| 
|2025.1	|[What Kind of Visual Tokens Do We Need? Training-free Visual Token Pruning for Multi-modal Large Language Models from the Perspective of Graph](https://arxiv.org/abs/2501.02268)	|Xiamen University	||
|2025.1	|[AdaFV: Accelerating VLMs with Self-Adaptive Cross-Modality Attention Mixture](https://arxiv.org/abs/2501.09532)|Inspur Genersoft Co. Ltd.||
|2025.1 |[FOLDER: Accelerating Multi-modal Large Language Models with Enhanced Performance](https://arxiv.org/abs/2501.02430)| Shanghai Jiao Tong University||


# 2024.12
|Date|Paper|Author|Conference|
|:---:|:---:|:---:|:---:| 
|2024.12|[[CLS] Attention is All You Need for Training-Free Visual Token Pruning: Make VLM Inference Faster](https://arxiv.org/abs/2412.01818)|Peking University|ICML2025|
|2024.12| [AIM: Adaptive Inference of Multi-Modal LLMs via Token Merging and Pruning](https://arxiv.org/abs/2412.03248)|The Chinese University of Hong Kong||


# 2024.11
|Date|Paper|Author|Conference|
|:---:|:---:|:---:|:---:| 
|2024.11|[Multi-Stage Vision Token Dropping: Towards Efficient Multimodal Large Language Model](https://arxiv.org/abs/2411.10803)|National University of Defense Technology||


# 2024.10
|Date|Paper|Author|Conference|
|:---:|:---:|:---:|:---:| 
|2024.10|[SparseVLM: Visual Token Sparsification for Efficient Vision-Language Model Inference](https://arxiv.org/abs/2410.04417)|Peking University|ICLR2025 Submission|
|2024.10|[Rethinking Visual Dependency in Long-Context Reasoning for Large Vision-Language Models](https://arxiv.org/abs/2410.19732)|University of Macau||
|2024.10|[Treat Visual Tokens as Text? But Your MLLM Only Needs Fewer Efforts to See](https://arxiv.org/abs/2410.06169)|University of Rochester||
|2024.10|[ZipVL: Efficient Large Vision-Language Models with Dynamic Token Sparsification](https://arxiv.org/abs/2410.08584)|Zhejiang University||

# 2024.9
|Date|Paper|Author|Conference|
|:---:|:---:|:---:|:---:| 
|2024.9|[Fit and Prune: Fast and Training-free Visual Token Pruning for Multi-modal Large Language Models](https://arxiv.org/abs/2409.10197)|Xiamen University	||
|2024.9|[Recoverable Compression: A Multimodal Vision Token Recovery Mechanism Guided by Text Information](https://arxiv.org/abs/2409.01179)|University of Chinese Academy of Sciences||
|2024.9|[Balancing Performance and Efficiency: A Multimodal Large Language Model Pruning Method based Image Text Interaction](https://arxiv.org/abs/2409.01162)|University of Chinese Academy of Sciences||

# 2024.3
|Date|Paper|Author|Conference|
|:---:|:---:|:---:|:---:| 
|2024.3|[LLaVA-PruMerge: Adaptive Token Reduction for Efficient Large Multimodal Models](https://arxiv.org/abs/2403.15388)|Illinois Institute of Technology|ICLR2025 Submission|


