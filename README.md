# Awesome-Vision-Token-Pruning-for-VLMs
# Accepted Paper
|Date|Paper|Author|Conference|
|:---:|:---:|:---:|:---:| 
|ECCV2024 accepted	|[IVTP: Instruction-Guided Visual Token Pruning for Large Vision-Language Models](https://link.springer.com/chapter/10.1007/978-3-031-72643-9_13)	|Alibaba Group	|ECCV2024 accepted|
|ECCV2024 accepted|[An Image is Worth 1/2 Tokens After Layer 2: Plug-and-Play Inference Acceleration for Large Vision-Language Models](https://link.springer.com/chapter/10.1007/978-3-031-73004-7_2)	|Peking University	|ECCV2024 accepted|
|ECCV2024 accepted|[Efficient Inference of Vision Instruction-Following Models with Elastic Cache](https://link.springer.com/chapter/10.1007/978-3-031-72643-9_4)	|Tsinghua University	|ECCV2024 accepted|
|ECCV2024 accepted|[Leveraging Pruning, Quantization and Multi-objective Optimization for an Efficient Deployment of Multi-modal Models](https://link.springer.com/chapter/10.1007/978-981-96-2074-6_5)|University of Science |ECCV2024 accepted|
|CVPR2024 accepted|[MADTP: Multimodal Alignment-Guided Dynamic Token Pruning for Accelerating Vision-Language Transformer](https://openaccess.thecvf.com/content/CVPR2024/html/Cao_MADTP_Multimodal_Alignment-Guided_Dynamic_Token_Pruning_for_Accelerating_Vision-Language_Transformer_CVPR_2024_paper.html)|Fudan University|CVPR2024 accepted|
|CVPR2024 accepted|[Zero-TPrune: Zero-Shot Token Pruning through Leveraging of the Attention Graph in Pre-Trained Transformers](https://openaccess.thecvf.com/content/CVPR2024/html/Wang_Zero-TPrune_Zero-Shot_Token_Pruning_through_Leveraging_of_the_Attention_Graph_CVPR_2024_paper.html)|Princeton University|CVPR2024 accepted|
# 2025.1
|Date|Paper|Author|Conference|
|:---:|:---:|:---:|:---:| 
|2025.1	|[What Kind of Visual Tokens Do We Need? Training-free Visual Token Pruning for Multi-modal Large Language Models from the Perspective of Graph](https://arxiv.org/abs/2501.02268)	|Xiamen University	||
|2025.1	|[AdaFV: Accelerating VLMs with Self-Adaptive Cross-Modality Attention Mixture](https://arxiv.org/abs/2501.09532)|Inspur Genersoft Co. Ltd.||
|2025.1 |[FOLDER: Accelerating Multi-modal Large Language Models with Enhanced Performance](https://arxiv.org/abs/2501.02430)| Shanghai Jiao Tong University||
# 2024.12
|Date|Paper|Author|Conference|
|:---:|:---:|:---:|:---:| 
|2024.12|[[CLS] Attention is All You Need for Training-Free Visual Token Pruning: Make VLM Inference Faster](https://arxiv.org/abs/2412.01818|Peking University)|ICML2025|
|2024.12| [AIM: Adaptive Inference of Multi-Modal LLMs via Token Merging and Pruning](https://arxiv.org/abs/2412.03248)|The Chinese University of Hong Kong||
# 2024.11
|Date|Paper|Author|Conference|
|:---:|:---:|:---:|:---:| 
|2024.11|[Multi-Stage Vision Token Dropping: Towards Efficient Multimodal Large Language Model](https://arxiv.org/abs/2411.10803)|National University of Defense Technology||

# 2024.10
|Date|Paper|Author|Conference|
|:---:|:---:|:---:|:---:| 
|2024.10|[SparseVLM: Visual Token Sparsification for Efficient Vision-Language Model Inference](https://arxiv.org/abs/2410.04417)|Peking University|ICLR2025 Submission|
|2024.10|[Rethinking Visual Dependency in Long-Context Reasoning for Large Vision-Language Models](https://arxiv.org/abs/2410.19732)|University of Macau||
|2024.10|[Treat Visual Tokens as Text? But Your MLLM Only Needs Fewer Efforts to See](https://arxiv.org/abs/2410.06169)|University of Rochester||

# 2024.9
|Date|Paper|Author|Conference|
|:---:|:---:|:---:|:---:| 
|2024.9|[Fit and Prune: Fast and Training-free Visual Token Pruning for Multi-modal Large Language Models](https://arxiv.org/abs/2409.10197)|Xiamen University	||
|2024.9|[Recoverable Compression: A Multimodal Vision Token Recovery Mechanism Guided by Text Information](https://arxiv.org/abs/2409.01179)|University of Chinese Academy of Sciences||
# 2024.3
|Date|Paper|Author|Conference|
|:---:|:---:|:---:|:---:| 
|2024.3|[LLaVA-PruMerge: Adaptive Token Reduction for Efficient Large Multimodal Models](https://arxiv.org/abs/2403.15388)|Illinois Institute of Technology|ICLR2025 Submission|
